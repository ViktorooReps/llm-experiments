from transformers import PretrainedConfig

from src.models.configuration_base import GPTBaseConfig


class FlexLlamaConfig(GPTBaseConfig):
    r"""
        Base class for all configuration classes. Handles a few parameters common to all models' configurations as well as
        methods for loading/downloading/saving configurations.

        <Tip>

        A configuration file can be loaded and saved to disk. Loading the configuration file and using this file to
        initialize a model does **not** load the model weights. It only affects the model's configuration.

        </Tip>

        Class attributes (overridden by derived classes):

        - **model_type** (`str`) -- An identifier for the model type, serialized into the JSON file, and used to recreate
          the correct object in [`~transformers.AutoConfig`].
        - **is_composition** (`bool`) -- Whether the config class is composed of multiple sub-configs. In this case the
          config has to be initialized from two or more configs of type [`~transformers.PretrainedConfig`] like:
          [`~transformers.EncoderDecoderConfig`] or [`~RagConfig`].
        - **keys_to_ignore_at_inference** (`List[str]`) -- A list of keys to ignore by default when looking at dictionary
          outputs of the model during inference.
        - **attribute_map** (`Dict[str, str]`) -- A dict that maps model specific attribute names to the standardized
          naming of attributes.

        Common attributes (present in all subclasses):

        - **vocab_size** (`int`) -- The number of tokens in the vocabulary, which is also the first dimension of the
          embeddings matrix (this attribute may be missing for models that don't have a text modality like ViT).
        - **hidden_size** (`int`) -- The hidden size of the model.
        - **num_attention_heads** (`int`) -- The number of attention heads used in the multi-head attention layers of the
          model.
        - **num_hidden_layers** (`int`) -- The number of blocks in the model.

        <Tip warning={true}>

        Setting parameters for sequence generation in the model config is deprecated. For backward compatibility, loading
        some of them will still be possible, but attempting to overwrite them will throw an exception -- you should set
        them in a [~transformers.GenerationConfig]. Check the documentation of [~transformers.GenerationConfig] for more
        information about the individual parameters.

        </Tip>

        Arg:
            add_block: bool = False,
            flex_block_size: int = 128,
            block_size: int = 32,

            vocab_size: int = 50304,
            hidden_size: int = 768,
            num_attention_heads: int = 12,
            num_hidden_layers: int = 12,
            sequence_length: int = 512,
            add_sink: bool = False,
            dropout: float = 0.0,
            bias: bool = False,
            rmsnorm_eps: float = 1e-5,

            name_or_path (`str`, *optional*, defaults to `""`):
                Store the string that was passed to [`PreTrainedModel.from_pretrained`] or
                [`TFPreTrainedModel.from_pretrained`] as `pretrained_model_name_or_path` if the configuration was created
                with such a method.
            output_hidden_states (`bool`, *optional*, defaults to `False`):
                Whether or not the model should return all hidden-states.
            output_attentions (`bool`, *optional*, defaults to `False`):
                Whether or not the model should returns all attentions.
            return_dict (`bool`, *optional*, defaults to `True`):
                Whether or not the model should return a [`~transformers.utils.ModelOutput`] instead of a plain tuple.
            is_encoder_decoder (`bool`, *optional*, defaults to `False`):
                Whether the model is used as an encoder/decoder or not.
            is_decoder (`bool`, *optional*, defaults to `False`):
                Whether the model is used as decoder or not (in which case it's used as an encoder).
            cross_attention_hidden_size** (`bool`, *optional*):
                The hidden size of the cross-attention layer in case the model is used as a decoder in an encoder-decoder
                setting and the cross-attention hidden dimension differs from `self.config.hidden_size`.
            add_cross_attention (`bool`, *optional*, defaults to `False`):
                Whether cross-attention layers should be added to the model. Note, this option is only relevant for models
                that can be used as decoder models within the [`EncoderDecoderModel`] class, which consists of all models
                in `AUTO_MODELS_FOR_CAUSAL_LM`.
            tie_encoder_decoder (`bool`, *optional*, defaults to `False`):
                Whether all encoder weights should be tied to their equivalent decoder weights. This requires the encoder
                and decoder model to have the exact same parameter names.
            prune_heads (`Dict[int, List[int]]`, *optional*, defaults to `{}`):
                Pruned heads of the model. The keys are the selected layer indices and the associated values, the list of
                heads to prune in said layer.

                For instance `{1: [0, 2], 2: [2, 3]}` will prune heads 0 and 2 on layer 1 and heads 2 and 3 on layer 2.
            chunk_size_feed_forward (`int`, *optional*, defaults to `0`):
                The chunk size of all feed forward layers in the residual attention blocks. A chunk size of `0` means that
                the feed forward layer is not chunked. A chunk size of n means that the feed forward layer processes `n` <
                sequence_length embeddings at a time. For more information on feed forward chunking, see [How does Feed
                Forward Chunking work?](../glossary.html#feed-forward-chunking).

            > Parameters for fine-tuning tasks

            architectures (`List[str]`, *optional*):
                Model architectures that can be used with the model pretrained weights.
            finetuning_task (`str`, *optional*):
                Name of the task used to fine-tune the model. This can be used when converting from an original (TensorFlow
                or PyTorch) checkpoint.
            id2label (`Dict[int, str]`, *optional*):
                A map from index (for instance prediction index, or target index) to label.
            label2id (`Dict[str, int]`, *optional*): A map from label to index for the model.
            num_labels (`int`, *optional*):
                Number of labels to use in the last layer added to the model, typically for a classification task.
            task_specific_params (`Dict[str, Any]`, *optional*):
                Additional keyword arguments to store for the current task.
            problem_type (`str`, *optional*):
                Problem type for `XxxForSequenceClassification` models. Can be one of `"regression"`,
                `"single_label_classification"` or `"multi_label_classification"`.

            > Parameters linked to the tokenizer

            tokenizer_class (`str`, *optional*):
                The name of the associated tokenizer class to use (if none is set, will use the tokenizer associated to the
                model by default).
            prefix (`str`, *optional*):
                A specific prompt that should be added at the beginning of each text before calling the model.
            bos_token_id (`int`, *optional*): The id of the _beginning-of-stream_ token.
            pad_token_id (`int`, *optional*): The id of the _padding_ token.
            eos_token_id (`int`, *optional*): The id of the _end-of-stream_ token.
            decoder_start_token_id (`int`, *optional*):
                If an encoder-decoder model starts decoding with a different token than _bos_, the id of that token.
            sep_token_id (`int`, *optional*): The id of the _separation_ token.

            > PyTorch specific parameters

            torchscript (`bool`, *optional*, defaults to `False`):
                Whether or not the model should be used with Torchscript.
            tie_word_embeddings (`bool`, *optional*, defaults to `True`):
                Whether the model's input and output word embeddings should be tied. Note that this is only relevant if the
                model has a output word embedding layer.
            torch_dtype (`str`, *optional*):
                The `dtype` of the weights. This attribute can be used to initialize the model to a non-default `dtype`
                (which is normally `float32`) and thus allow for optimal storage allocation. For example, if the saved
                model is `float16`, ideally we want to load it back using the minimal amount of memory needed to load
                `float16` weights. Since the config object is stored in plain text, this attribute contains just the
                floating type string without the `torch.` prefix. For example, for `torch.float16` ``torch_dtype` is the
                `"float16"` string.

                This attribute is currently not being used during model loading time, but this may change in the future
                versions. But we can already start preparing for the future by saving the dtype with save_pretrained.

            > TensorFlow specific parameters

            use_bfloat16 (`bool`, *optional*, defaults to `False`):
                Whether or not the model should use BFloat16 scalars (only used by some TensorFlow models).
            tf_legacy_loss (`bool`, *optional*, defaults to `False`):
                Whether the model should use legacy TensorFlow losses. Legacy losses have variable output shapes and may
                not be XLA-compatible. This option is here for backward compatibility and will be removed in Transformers
                v5.
    """
    model_type = 'flex-llama'

    def __init__(
        self,
        add_block: bool = False,
        flex_block_size: int = 128,
        block_size: int = 32,

        vocab_size: int = 50304,
        hidden_size: int = 768,
        num_attention_heads: int = 12,
        num_hidden_layers: int = 12,
        sequence_length: int = 512,
        add_sink: bool = False,
        dropout: float = 0.0,
        bias: bool = False,
        rmsnorm_eps: float = 1e-5,
        **kwargs
    ):
        super().__init__(
            vocab_size=vocab_size,
            hidden_size=hidden_size,
            num_attention_heads=num_attention_heads,
            num_hidden_layers=num_hidden_layers,
            sequence_length=sequence_length,
            add_sink=add_sink,
            dropout=dropout,
            bias=bias,
            **kwargs
        )

        self.add_block = add_block
        self.flex_block_size = flex_block_size
        self.block_size = block_size

        self.block_end_token = None
        self.sink_token = None
        self.pad_token = None
